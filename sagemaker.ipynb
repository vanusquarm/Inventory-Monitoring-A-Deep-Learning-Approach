{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Monitoring at Distribution Centers \n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The goal of this project is to build a pipeline to process real-world, user-supplied images. \n",
    "Particularly, this project classifies a bin image by the number of objects present using \n",
    "Convolutional Neural Network (CNN). Given an inventory bin image, the algorithm will identify an \n",
    "estimate of the count of objects in the image through classification based on the Amazon Bin Image datasets the model is trained on.\n",
    "\n",
    "• Inventory monitoring enables the maximum amount of profit from the least amount of investment in inventory without affecting customer satisfaction. Done right, it can help avoid problems, such as out-of-stock (stockout) events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "\n",
    "The cell below creates folders `train` and `test`, downloads image data and arranges it in subfolders. Each of these subfolders contain images where the number of objects is equal to the name of the folder. For instance, all images in folder `1` has images with 1 object in them. Images are not divided into training, testing or validation sets. If you feel like the number of samples are not enough, you can always download more data (instructions for that can be found [here](https://registry.opendata.aws/amazon-bin-imagery/)). However, we are not acessing you on the accuracy of your final trained model, but how you create your machine learning engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Retrieve the sagemaker session\n",
    "sagemaker_session=sagemaker.Session()\n",
    "\n",
    "# Retrieve the sagemaker S3 bucket, region and role\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(\"Default Bucket: {}\".format(bucket))\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_and_arrange_data():\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    with open('file_list.json', 'r') as f:\n",
    "        d=json.load(f)\n",
    "        temp = d\n",
    "    for data_path in [\"train\",\"test\"]:\n",
    "        for k, v in d.items():\n",
    "            d.update({k:v[0:1024]}) if (data_path == \"train\") else d.update({k:v[512:-1]})\n",
    "            print(f\"Downloading Images with {k} objects\")\n",
    "            directory=os.path.join('data_path', k)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            for file_path in tqdm(v):\n",
    "                file_name=os.path.basename(file_path).split('.')[0]+'.jpg'\n",
    "                s3_client.download_file('aft-vbi-pds', os.path.join('bin-images', file_name),\n",
    "                                os.path.join(directory, file_name))\n",
    "        d = temp\n",
    "\n",
    "download_and_arrange_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "\n",
    "### Overview of Dataset and Inputs used\n",
    "\n",
    "• Data can be downloaded from Amazon Open Data website https://registry.opendata.aws/amazon-bin-imagery/\n",
    "• Data is captured by Amazon in their Fulfilment centre and has around 50000 images\n",
    "• License\n",
    "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States (CC BY-NC-SA 3.0 US) https://creativecommons.org/licenses/by-nc-sa/3.0/us/\n",
    "• Images are located in the bin-images directory, and metadata for each image is located in the metadata directory. Images and their associated metadata share simple numerical unique identifiers.\n",
    "Inputs\n",
    "There are two set of inputs for the model training\n",
    "1. Images for the model, which is available in the source as JPEG file\n",
    "Example\n",
    "https://aft-vbi-pds.s3.amazonaws.com/bin-images/1005.jpg\n",
    "2. JSON format with meta data for the image\n",
    "Example https://aft-vbi-pds.s3.amazonaws.com/metadata/1005.json.\n",
    "\n",
    "From the JSON file, we can filter the target label which the quantity of the objects in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "request_dict={ \"url\": \"https://aft-vbi-pds.s3.amazonaws.com/bin-images/512.jpg\" }\n",
    "img_bytes = requests.get(request_dict['url']).content\n",
    "Image.open(io.BytesIO(img_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!aws s3 cp train s3://bucket/train/ --recursive\n",
    "!aws s3 cp test s3://bucket/test/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "This is the part where you can train a model. The type or architecture of the model you use is not important. \n",
    "\n",
    "**Note:** You will need to use the `train.py` script to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare your model training hyperparameter.\n",
    "#NOTE: You do not need to do hyperparameter tuning. You can use fixed hyperparameter values\n",
    "\n",
    "hyperparameters = { \"batch-size\": 64,\n",
    "                   \"epochs\": 10,\n",
    "                   \"lr\": 0.05}\n",
    "hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point = \"train.py\",\n",
    "    base_job_name = \"bin-imgclassification\",\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.g4dn.2xlarge\",\n",
    "    hyperparameters = hyperparameters,\n",
    "    framework_version = \"1.8\",\n",
    "    py_version = \"py36\",\n",
    "    output_path = f\"s3://{bucket}/output/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fit estimator\n",
    "train_data = f's3://{bucket}/train/'\n",
    "test_data = f's3://{bucket}/test/'\n",
    "estimator.fit({'train': train_data, 'test':test_data}, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standout Suggestions\n",
    "You do not need to perform the tasks below to finish your project. However, you can attempt these tasks to turn your project into a more advanced portfolio piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Perform hyperparameter tuning to increase the performance of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "#TODO: Create your hyperparameter search space\n",
    "hyperparameters = {\"test-batch-size\": \"128\"}\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.011),\n",
    "    \"batch-size\": CategoricalParameter([32, 64, 128, 256, 512]),\n",
    "    \"epochs\": IntegerParameter(2,14)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point = \"hpo.py\",\n",
    "    base_job_name = \"hptuning-abid\",\n",
    "    role = role,\n",
    "    instance_count = 5,\n",
    "    instance_type = \"ml.g4dn.2xlarge\", #\"ml.m5.large\"\n",
    "    hyperparameters = hyperparameters,\n",
    "    framework_version = \"1.8\",\n",
    "    py_version = \"py36\",\n",
    "    output_path = f\"s3://{bucket}/hpo-output/\"\n",
    ")\n",
    "\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]\n",
    "\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs = 2,\n",
    "    max_parallel_jobs = 2,\n",
    "    objective_type = objective_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your estimator\n",
    "tuner.fit({'train': train_data,\n",
    "           'test': test_data}, wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Find the best hyperparameters\n",
    "best_estimator = tuner.best_estimator()\n",
    "\n",
    "#Get the hyperparameters of the best trained model\n",
    "best_hyperparameters = best_estimator.hyperparameters()\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Profiling and Debugging\n",
    "Using the best hyperparameters, model debugging, profiling to better monitor and debug, and finetune the model training job.\n",
    "Note: You will need to use the train_model.py script to perform model profiling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Set up debugging and profiling rules and hooks\n",
    "from sagemaker.debugger import Rule, ProfilerRule, DebuggerHookConfig, rule_configs\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "\n",
    "rules = [\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit())\n",
    "]\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")\n",
    "\n",
    "hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\n",
    "        \"train.save_interval\": \"10\",\n",
    "        \"eval.save_interval\": \"1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Create and fit an estimator\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_model.py\",\n",
    "    base_job_name='objectCountEstimator',\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    hyperparameters=best_hyperparameters,\n",
    "    rules = rules,\n",
    "    debugger_hook_config=hook_config,\n",
    "    profiler_config=profiler_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"train\": train_data, \"test\": test_data}, wait=True) #   Your estimator here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#   Plot a debugging output.\n",
    "import smdebug\n",
    "\n",
    "job_name = estimator.latest_training_job.name\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "trial = smdebug.trials.create_trial(estimator.latest_job_debugger_artifacts_path())\n",
    "\n",
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.core.modes import ModeKeys\n",
    "\n",
    "len(trial.tensor(\"CrossEntropyLoss_output_0\").steps(mode=ModeKeys.TRAIN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(trial.tensor(\"CrossEntropyLoss_output_0\").steps(mode=ModeKeys.EVAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(trial, tensor_name, mode):\n",
    "    tensor = trial.tensor(tensor_name)\n",
    "    steps = tensor.steps(mode=mode)\n",
    "    vals = [tensor.value(s, mode=mode) for s in steps]\n",
    "    return steps, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "\n",
    "def plot_tensor(trial, tensor_name):\n",
    "\n",
    "    steps_train, vals_train = get_data(trial, tensor_name, mode=ModeKeys.TRAIN)\n",
    "    print(\"loaded TRAIN data\")\n",
    "    steps_eval, vals_eval = get_data(trial, tensor_name, mode=ModeKeys.EVAL)\n",
    "    print(\"loaded EVAL data\")\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    host = host_subplot(111)\n",
    "\n",
    "    par = host.twiny()\n",
    "\n",
    "    host.set_xlabel(\"Steps (TRAIN)\")\n",
    "    par.set_xlabel(\"Steps (EVAL)\")\n",
    "    host.set_ylabel(tensor_name)\n",
    "\n",
    "    (p1,) = host.plot(steps_train, vals_train, label=tensor_name)\n",
    "    print(\"completed TRAIN plot\")\n",
    "    (p2,) = par.plot(steps_eval, vals_eval, label=\"val_\" + tensor_name)\n",
    "    print(\"completed EVAL plot\")\n",
    "    leg = plt.legend()\n",
    "\n",
    "    host.xaxis.get_label().set_color(p1.get_color())\n",
    "    leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "    par.xaxis.get_label().set_color(p2.get_color())\n",
    "    leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "    plt.ylabel(tensor_name)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tensor(trial, \"CrossEntropyLoss_output_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Display the profiler output\n",
    "import os\n",
    "\n",
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")\n",
    "!aws s3 ls {rule_output_path} --recursive\n",
    "\n",
    "## copy to local\n",
    "!aws s3 cp {rule_output_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get the autogenerated folder name of profiler report\n",
    "profiler_report_name = [\n",
    "    rule[\"RuleConfigurationName\"]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"Profiler\" in rule[\"RuleConfigurationName\"]\n",
    "][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.HTML(filename=profiler_report_name + \"/profiler-output/profiler-report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deploying and Querying\n",
    "Deploy your model to an endpoint and then query that endpoint to get a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location=estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=estimator.model_data, \n",
    "                             role=role, \n",
    "                             entry_point='inference.py', \n",
    "                             py_version='py36',\n",
    "                             framework_version='1.8')\n",
    "\n",
    "\n",
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "predictor.serializer = IdentitySerializer(\"image/jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "\n",
    "def identify_bin_type(image_url):\n",
    "    img_bytes = requests.get(image_url).content  #get image bytes in any format\n",
    "    image = Image.open(io.BytesIO(img_bytes)) #convert bytes to file object and open\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    image.save(buf, format=\"JPEG\") #convert file to jpeg BytesIO object\n",
    "    response = predictor.predict(buf.getvalue()) #get the bytes from the object and pass it to the predict function\n",
    "    \n",
    "    return image, response\n",
    "\n",
    "url = \"https://aft-vbi-pds.s3.amazonaws.com/bin-images/777.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _ = identify_bin_type(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, response = identify_bin_type(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bin_type = np.argmax(response, 1).item()\n",
    "\n",
    "print(f\"The image is of bin-type{bin_type+1} with {bin_type+1} objects present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheaper Training and Cost Analysis\n",
    "Perform a cost analysis of your system and then use spot instances to lessen your model training cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis\n",
    "\n",
    "Sagmaker vs EC2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker\n",
    "\n",
    "\n",
    "### Sagemaker cost items are as follows\n",
    "\n",
    "- GB-Mo of Training Job ML storage\n",
    "- Studio-Notebook ml.t3.medium hour in US East (N. Virginia)\n",
    "- Training ml.m5.xlarge hour in US East (N. Virginia)\n",
    "- SageMaker Studio Notebook Instance ml.g4dn.xlarge per hour\n",
    "\n",
    "\n",
    "### Overall cost for SageMaker usage - $6.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"sagemaker-cost.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EC2 Model cost Items\n",
    "\n",
    "- Amazon Elastic Compute Cloud running Linux/UNIX - Demand Linux t2.medium Instance \n",
    "\n",
    "- General Purpose SSD (gp2) provisioned storage\n",
    "\n",
    "### Overall cost for SageMaker usage - $0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"ec2train-cost.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training cost in EC2 is less compared to SageMaker. \n",
    "However, higher instance cost and usage will be a factor on considering the workspace "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "fe44fef87f92f48a3a32707d0df204585f471652bc0ce87358a3ce712bc24db0"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
