
Inventory Monitoring at Distribution Centers
Domain Overview

Distribution centers form the critical part of a supply chain. Goods are transported in and out of these distribution centers to the destination such as wholesale, retail stores, customer warehouse etc. Goods transported need to be efficiently handled and this applies for almost all industries irrespective of products manufactured. In order to streamline the process, company employ inventory monitoring steps to make sure right goods are moved to the right place.

• Improving inventory monitoring will result in streamlining entire supply chain and also improves customer and vendor management and relationship for an organization • In this context, this project focus on how to implement Machine Learning +Computer vision to modernize and improve the inventory monitoring process
Image Source

• Image source is from a high-resolution camera and images of the goods, bundle and other components which forms part of inventory that are moved in the distribution centers. • The technical details on high-end image capture are outside the scope of this project. • As part of the solution, we will be used the pre-built data which is available in • Amazon Bin Image Dataset - Registry of Open Data on AWS • The data will be loaded into the S3 bucket of AWS cloud for consumption in the model training
Algorithm

• Algorithm that will be used as part of the solution is based on Convolution Neural Network architecture where the images are feed into the network to train the model and will be used for final classification based on the quantity of the goods in the image. • PyTorch Deep Learning Framework will be used • AWS Sagemaker instance will be initiated and image that is stored in S3 bucket will be used as part of data pipeline. • Model will be tuned and model with best hyper parameter will be identified and deployed

import argparse
import numpy as np
import json
import logging
import os
import io
import sys
import matplotlib.pyplot as plt
import requests
from PIL import Image
import json

!pip install torch

Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (1.4.0)

!pip install torchvision

Requirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (0.5.0+cpu)
Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchvision) (1.4.0)
Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision) (1.16.4)
Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision) (8.1.2)
Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision) (1.16.0)

import boto3
import torch
import sagemaker
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.models as models
import torchvision.transforms as transforms
from torchvision import datasets, transforms
from torch.utils import data
from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner
from sagemaker.pytorch import PyTorch
from sagemaker import get_execution_role
from sagemaker.debugger import Rule, DebuggerHookConfig, TensorBoardOutputConfig, CollectionConfig, ProfilerRule, rule_configs
from sagemaker.debugger import ProfilerConfig, FrameworkProfile

Data Preparation

The cell below creates a folder called train_data, downloads training data and arranges it in subfolders. Each of these subfolders contain images where the number of objects is equal to the name of the folder. For instance, all images in folder 1 has images with 1 object in them. Images are not divided into training, testing or validation sets. If you feel like the number of samples are not enough, you can always download more data (instructions for that can be found here). However, we are not acessing you on the accuracy of your final trained model, but how you create your machine learning engineering pipeline.

import os
import json
import boto3
from tqdm import tqdm

def download_and_arrange_data():
    s3_client = boto3.client('s3')

    with open('file_list.json', 'r') as f:
        d=json.load(f)

    for k, v in d.items():
        print(f"Downloading Images with {k} objects")
        directory=os.path.join('train_data', k)
        if not os.path.exists(directory):
            os.makedirs(directory)
        for file_path in tqdm(v):
            file_name=os.path.basename(file_path).split('.')[0]+'.jpg'
            s3_client.download_file('aft-vbi-pds', os.path.join('bin-images', file_name),
                             os.path.join(directory, file_name))

download_and_arrange_data()

  0%|          | 1/1228 [00:00<02:48,  7.29it/s]

Downloading Images with 1 objects

100%|██████████| 1228/1228 [02:20<00:00,  8.73it/s]
  0%|          | 1/2299 [00:00<06:05,  6.28it/s]

Downloading Images with 2 objects

100%|██████████| 2299/2299 [04:26<00:00,  8.62it/s]
  0%|          | 0/2666 [00:00<?, ?it/s]

Downloading Images with 3 objects

100%|██████████| 2666/2666 [05:08<00:00,  8.65it/s]
  0%|          | 2/2373 [00:00<03:41, 10.69it/s]

Downloading Images with 4 objects

100%|██████████| 2373/2373 [04:33<00:00,  8.67it/s]
  0%|          | 0/1875 [00:00<?, ?it/s]

Downloading Images with 5 objects

100%|██████████| 1875/1875 [03:35<00:00,  8.69it/s]

Dataset
Overview of Dataset and Inputs used

• Data can be downloaded from Amazon Open Data website https://registry.opendata.aws/amazon-bin-imagery/ • Data is captured by Amazon in their Fulfilment centre and has around 500000 images • License Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States (CC BY-NC-SA 3.0 US) https://creativecommons.org/licenses/by-nc-sa/3.0/us/ • Images are located in the bin-images directory, and metadata for each image is located in the metadata directory. Images and their associated metadata share simple numerical unique identifiers. Inputs There are two set of inputs for the model training

    Images for the model, which is available in the source as JPEG file Example https://aft-vbi-pds.s3.amazonaws.com/bin-images/523.jpg
    JSON format with meta data for the image Example https://aft-vbi-pds.s3.amazonaws.com/metadata/523.json. From the JSON file, we can filter the target label which the quantity of the objects in the image

request_dict={ "url": "https://aft-vbi-pds.s3.amazonaws.com/bin-images/559.jpg" }
img_bytes = requests.get(request_dict['url']).content
Image.open(io.BytesIO(img_bytes))

os.environ['SM_CHANNEL_TRAINING']='s3://udacitycapstone9/'
os.environ['SM_MODEL_DIR']='s3://udacitycapstone9/model/'
os.environ['SM_OUTPUT_DATA_DIR']='s3://udacitycapstone9/output/'

%%capture
!aws s3 cp train_data s3://udacitycapstone9/ --recursive

role = sagemaker.get_execution_role()

Couldn't call 'get_role' to get Role ARN from role name AmazonSageMaker-ExecutionRole-20211102T122677 to get Role path.
Assuming role was created in SageMaker AWS console, as the name contains `AmazonSageMaker-ExecutionRole`. Defaulting to Role ARN with service-role in path. If this Role ARN is incorrect, please add IAM read permissions to your role or supply the Role Arn directly.

sagemaker_session = sagemaker.Session()

Model Training

TODO: This is the part where you can train a model. The type or architecture of the model you use is not important.

Note: You will need to use the train.py script to train your model.

#TODO: Declare your model training hyperparameter.
#NOTE: You do not need to do hyperparameter tuning. You can use fixed hyperparameter values




hyperparameter_ranges = {
    "learning_rate": ContinuousParameter(0.001, 0.1),
    "batch_size": CategoricalParameter([32, 64, 128]),
    "epochs": CategoricalParameter([10,15, 25 , 30 ])
}

role = sagemaker.get_execution_role()

objective_metric_name = "Test Loss"
objective_type = "Minimize"
metric_definitions = [{"Name": "Test Loss", "Regex": "Testing Loss: ([0-9\\.]+)"}]

Couldn't call 'get_role' to get Role ARN from role name AmazonSageMaker-ExecutionRole-20211102T122677 to get Role path.
Assuming role was created in SageMaker AWS console, as the name contains `AmazonSageMaker-ExecutionRole`. Defaulting to Role ARN with service-role in path. If this Role ARN is incorrect, please add IAM read permissions to your role or supply the Role Arn directly.

estimator = PyTorch(
    entry_point="tuner.py",
    base_job_name='pytorch_hpo',
    role=role,
    framework_version="1.4.0",
    instance_count=1,
    instance_type="ml.m5.xlarge",
    py_version='py3'
)

tuner = HyperparameterTuner(
    estimator,
    objective_metric_name,
    hyperparameter_ranges,
    metric_definitions,
    max_jobs=2,
    max_parallel_jobs=2,
    objective_type=objective_type
)

tuner.fit({"training": "s3://udacitycapstone9/"})

tuner.describe()

{'HyperParameterTuningJobName': 'pytorch-training-211214-1932',
 'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-1:701620947472:hyper-parameter-tuning-job/pytorch-training-211214-1932',
 'HyperParameterTuningJobConfig': {'Strategy': 'Bayesian',
  'HyperParameterTuningJobObjective': {'Type': 'Minimize',
   'MetricName': 'Test Loss'},
  'ResourceLimits': {'MaxNumberOfTrainingJobs': 2,
   'MaxParallelTrainingJobs': 2},
  'ParameterRanges': {'IntegerParameterRanges': [],
   'ContinuousParameterRanges': [{'Name': 'learning_rate',
     'MinValue': '0.001',
     'MaxValue': '0.1',
     'ScalingType': 'Auto'}],
   'CategoricalParameterRanges': [{'Name': 'batch_size',
     'Values': ['"32"', '"64"', '"128"']},
    {'Name': 'epochs', 'Values': ['"10"', '"15"', '"25"', '"30"']}]},
  'TrainingJobEarlyStoppingType': 'Off'},
 'TrainingJobDefinition': {'StaticHyperParameters': {'_tuning_objective_metric': 'Test Loss',
   'sagemaker_container_log_level': '20',
   'sagemaker_estimator_class_name': '"PyTorch"',
   'sagemaker_estimator_module': '"sagemaker.pytorch.estimator"',
   'sagemaker_job_name': '"pytorch_hpo-2021-12-14-19-32-35-160"',
   'sagemaker_program': '"tuner.py"',
   'sagemaker_region': '"us-east-1"',
   'sagemaker_submit_directory': '"s3://sagemaker-us-east-1-701620947472/pytorch_hpo-2021-12-14-19-32-35-160/source/sourcedir.tar.gz"'},
  'AlgorithmSpecification': {'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.4.0-cpu-py3',
   'TrainingInputMode': 'File',
   'MetricDefinitions': [{'Name': 'Test Loss',
     'Regex': 'Testing Loss: ([0-9\\.]+)'},
    {'Name': 'ObjectiveMetric', 'Regex': 'Testing Loss: ([0-9\\.]+)'}]},
  'RoleArn': 'arn:aws:iam::701620947472:role/service-role/AmazonSageMaker-ExecutionRole-20211102T122677',
  'InputDataConfig': [{'ChannelName': 'training',
    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',
      'S3Uri': 's3://udacitycapstone9/',
      'S3DataDistributionType': 'FullyReplicated'}}}],
  'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-701620947472/'},
  'ResourceConfig': {'InstanceType': 'ml.m5.xlarge',
   'InstanceCount': 1,
   'VolumeSizeInGB': 30},
  'StoppingCondition': {'MaxRuntimeInSeconds': 86400},
  'EnableNetworkIsolation': False,
  'EnableInterContainerTrafficEncryption': False,
  'EnableManagedSpotTraining': False},
 'HyperParameterTuningJobStatus': 'Completed',
 'CreationTime': datetime.datetime(2021, 12, 14, 19, 32, 35, 691000, tzinfo=tzlocal()),
 'HyperParameterTuningEndTime': datetime.datetime(2021, 12, 14, 19, 58, 0, 323000, tzinfo=tzlocal()),
 'LastModifiedTime': datetime.datetime(2021, 12, 14, 19, 58, 0, 323000, tzinfo=tzlocal()),
 'TrainingJobStatusCounters': {'Completed': 2,
  'InProgress': 0,
  'RetryableError': 0,
  'NonRetryableError': 0,
  'Stopped': 0},
 'ObjectiveStatusCounters': {'Succeeded': 2, 'Pending': 0, 'Failed': 0},
 'BestTrainingJob': {'TrainingJobName': 'pytorch-training-211214-1932-002-3f0e8060',
  'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:701620947472:training-job/pytorch-training-211214-1932-002-3f0e8060',
  'CreationTime': datetime.datetime(2021, 12, 14, 19, 32, 49, tzinfo=tzlocal()),
  'TrainingStartTime': datetime.datetime(2021, 12, 14, 19, 35, 19, tzinfo=tzlocal()),
  'TrainingEndTime': datetime.datetime(2021, 12, 14, 19, 56, 24, tzinfo=tzlocal()),
  'TrainingJobStatus': 'Completed',
  'TunedHyperParameters': {'batch_size': '"128"',
   'epochs': '"15"',
   'learning_rate': '0.017454935661745122'},
  'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'Test Loss',
   'Value': 49.0},
  'ObjectiveStatus': 'Succeeded'},
 'ResponseMetadata': {'RequestId': 'e14d48b7-c156-4b6b-9778-2d6d4a29b496',
  'HTTPStatusCode': 200,
  'HTTPHeaders': {'x-amzn-requestid': 'e14d48b7-c156-4b6b-9778-2d6d4a29b496',
   'content-type': 'application/x-amz-json-1.1',
   'content-length': '3153',
   'date': 'Tue, 14 Dec 2021 20:00:40 GMT'},
  'RetryAttempts': 0}}

tuner.describe().keys()

dict_keys(['HyperParameterTuningJobName', 'HyperParameterTuningJobArn', 'HyperParameterTuningJobConfig', 'TrainingJobDefinition', 'HyperParameterTuningJobStatus', 'CreationTime', 'HyperParameterTuningEndTime', 'LastModifiedTime', 'TrainingJobStatusCounters', 'ObjectiveStatusCounters', 'BestTrainingJob', 'ResponseMetadata'])

lengths

[5221, 2611, 2609]

tuner.describe()['HyperParameterTuningJobName']

'pytorch-training-211214-1932'

from sagemaker.analytics import HyperparameterTuningJobAnalytics

exp = HyperparameterTuningJobAnalytics(
  hyperparameter_tuning_job_name=tuner.describe()['HyperParameterTuningJobName'])

jobs = exp.dataframe()

jobs.sort_values('FinalObjectiveValue', ascending=0)

	batch_size 	epochs 	learning_rate 	TrainingJobName 	TrainingJobStatus 	FinalObjectiveValue 	TrainingStartTime 	TrainingEndTime 	TrainingElapsedTimeSeconds
1 	"32" 	"25" 	0.067109 	pytorch-training-211214-1932-001-39a1db98 	Completed 	50.0 	2021-12-14 19:35:36+00:00 	2021-12-14 19:57:56+00:00 	1340.0
0 	"128" 	"15" 	0.017455 	pytorch-training-211214-1932-002-3f0e8060 	Completed 	49.0 	2021-12-14 19:35:19+00:00 	2021-12-14 19:56:24+00:00 	1265.0

TrainingJobName= 'pytorch-training-211214-1932-002-3f0e8060'

estimator = sagemaker.estimator.Estimator.attach(TrainingJobName)

2021-12-14 19:56:24 Starting - Preparing the instances for training
2021-12-14 19:56:24 Downloading - Downloading input data
2021-12-14 19:56:24 Training - Training image download completed. Training in progress.
2021-12-14 19:56:24 Uploading - Uploading generated training model
2021-12-14 19:56:24 Completed - Training job completed

best_estimator=estimator

estimator.hyperparameters()

{'batch_size': '128',
 'learning_rate': '"\\"0.017454935661745122\\""',
 'epochs': '15',
 'sagemaker_submit_directory': '"s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-18-39-925/source/sourcedir.tar.gz"',
 'sagemaker_program': '"train.py"',
 'sagemaker_container_log_level': '20',
 'sagemaker_job_name': '"Training-job-2021-12-15-15-18-39-925"',
 'sagemaker_region': '"us-east-1"'}

hyperparameters = {"batch_size": int(estimator.hyperparameters()['batch_size'].replace('"', '')), \
                   "learning_rate": 0.01745493566174512, \
                   "epochs" : 15
                                  }
hyperparameters

{'batch_size': 128, 'learning_rate': 0.01745493566174512, 'epochs': 15}

Model Profiling and Debugging

TODO: Use model debugging and profiling to better monitor and debug your model training job.

rules = [
    Rule.sagemaker(rule_configs.vanishing_gradient()),
    Rule.sagemaker(rule_configs.overfit()),
    Rule.sagemaker(rule_configs.overtraining()),
    Rule.sagemaker(rule_configs.poor_weight_initialization()),
    Rule.sagemaker(base_config=rule_configs.loss_not_decreasing(),
                             rule_parameters={"tensor_regex": "CrossEntropyLoss_output_0",
                                             "mode": "TRAIN"}),
    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),
]

hook_config = DebuggerHookConfig(
    hook_parameters={
        "train.save_interval": "10",
        "eval.save_interval": "1",
    }
)


profiler_config = ProfilerConfig(
    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=5)
)

from sagemaker.debugger import Rule, ProfilerRule, rule_configs, DebuggerHookConfig, ProfilerConfig, FrameworkProfile, CollectionConfig

rules = [
    Rule.sagemaker(rule_configs.loss_not_decreasing()),
    Rule.sagemaker(rule_configs.overfit()),
    Rule.sagemaker(rule_configs.overtraining()),
    Rule.sagemaker(rule_configs.poor_weight_initialization()),
    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),
    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),
]

profiler_config = ProfilerConfig(
    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)
)

collection_config_list = [
    CollectionConfig(
        name="CrossEntropyLoss_output_0",
        parameters={
            "include_regex": "CrossEntropyLoss_output_0", 
            "train.save_interval": "10",
            "eval.save_interval": "1"
        }
    )
]


hook_config = DebuggerHookConfig(
    # hook_parameters={"train.save_interval": "100", "eval.save_interval": "10"}
    collection_configs=collection_config_list
)

estimator = PyTorch(
    entry_point='train.py',
    base_job_name='Training-job',
    role=role,
    instance_count=1,
    instance_type='ml.m5.xlarge',
    framework_version='1.4.0',
    py_version='py3',
    hyperparameters=hyperparameters,
    rules = rules,
    debugger_hook_config=hook_config,
    profiler_config=profiler_config,
)

estimator.fit({"training": "s3://udacitycapstone9/"}, wait=True)

2021-12-15 15:55:12 Starting - Starting the training job...
2021-12-15 15:55:20 Starting - Launching requested ML instancesLossNotDecreasing: InProgress
Overfit: InProgress
Overtraining: InProgress
PoorWeightInitialization: InProgress
LowGPUUtilization: InProgress
ProfilerReport: InProgress
......
2021-12-15 15:56:28 Starting - Preparing the instances for training.........
2021-12-15 15:58:09 Downloading - Downloading input data............
2021-12-15 16:00:13 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
2021-12-15 16:00:15,191 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training
2021-12-15 16:00:15,194 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 16:00:15,205 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.
2021-12-15 16:00:18,222 sagemaker_pytorch_container.training INFO     Invoking user training script.
2021-12-15 16:00:25,782 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. 
Generating setup.py
2021-12-15 16:00:25,782 sagemaker-containers INFO     Generating setup.cfg
2021-12-15 16:00:25,782 sagemaker-containers INFO     Generating MANIFEST.in
2021-12-15 16:00:25,782 sagemaker-containers INFO     Installing module with the following command:
/opt/conda/bin/python3.6 -m pip install . 
Processing /tmp/tmpdg9zlu7j/module_dir
Building wheels for collected packages: default-user-module-name
  Building wheel for default-user-module-name (setup.py): started
  Building wheel for default-user-module-name (setup.py): finished with status 'done'
  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=6569 sha256=54b3e3ed628597821eb0b2ff14270453984dfe4c838da723839d4bc0e93f6d0a
  Stored in directory: /tmp/pip-ephem-wheel-cache-hvq2k7n1/wheels/ee/e9/64/b42222a92ebdcdb739cdbb7aaca08a549f54613a298806b2bd
Successfully built default-user-module-name
Installing collected packages: default-user-module-name
Successfully installed default-user-module-name-1.0.0
2021-12-15 16:00:28,289 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 16:00:28,302 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 16:00:28,314 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 16:00:28,324 sagemaker-containers INFO     Invoking user script
Training Env:
{
    "additional_framework_parameters": {},
    "channel_input_dirs": {
        "training": "/opt/ml/input/data/training"
    },
    "current_host": "algo-1",
    "framework_module": "sagemaker_pytorch_container.training:main",
    "hosts": [
        "algo-1"
    ],
    "hyperparameters": {
        "batch_size": 128,
        "epochs": 15,
        "learning_rate": 0.01745493566174512
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {
        "training": {
            "TrainingInputMode": "File",
            "S3DistributionType": "FullyReplicated",
            "RecordWrapperType": "None"
        }
    },
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "Training-job-2021-12-15-15-55-11-903",
    "log_level": 20,
    "master_hostname": "algo-1",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/source/sourcedir.tar.gz",
    "module_name": "train",
    "network_interface_name": "eth0",
    "num_cpus": 4,
    "num_gpus": 0,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1",
        "hosts": [
            "algo-1"
        ],
        "network_interface_name": "eth0"
    },
    "user_entry_point": "train.py"
}
Environment variables:
SM_HOSTS=["algo-1"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"batch_size":128,"epochs":15,"learning_rate":0.01745493566174512}
SM_USER_ENTRY_POINT=train.py
SM_FRAMEWORK_PARAMS={}
SM_RESOURCE_CONFIG={"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"}
SM_INPUT_DATA_CONFIG={"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=["training"]
SM_CURRENT_HOST=algo-1
SM_MODULE_NAME=train
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=4
SM_NUM_GPUS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{},"channel_input_dirs":{"training":"/opt/ml/input/data/training"},"current_host":"algo-1","framework_module":"sagemaker_pytorch_container.training:main","hosts":["algo-1"],"hyperparameters":{"batch_size":128,"epochs":15,"learning_rate":0.01745493566174512},"input_config_dir":"/opt/ml/input/config","input_data_config":{"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}},"input_dir":"/opt/ml/input","is_master":true,"job_name":"Training-job-2021-12-15-15-55-11-903","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/source/sourcedir.tar.gz","module_name":"train","network_interface_name":"eth0","num_cpus":4,"num_gpus":0,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"},"user_entry_point":"train.py"}
SM_USER_ARGS=["--batch_size","128","--epochs","15","--learning_rate","0.01745493566174512"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_CHANNEL_TRAINING=/opt/ml/input/data/training
SM_HP_BATCH_SIZE=128
SM_HP_EPOCHS=15
SM_HP_LEARNING_RATE=0.01745493566174512
PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages
Invoking script with the following command:
/opt/conda/bin/python3.6 train.py --batch_size 128 --epochs 15 --learning_rate 0.01745493566174512
Namespace(batch_size=128, data='/opt/ml/input/data/training', epochs=15, learning_rate=0.01745493566174512, model_dir='/opt/ml/model', output_dir='/opt/ml/output/data')
Hyperparameters are LR: 0.01745493566174512, Batch Size: 128
Data Paths: /opt/ml/input/data/training
[2021-12-15 16:00:30.983 algo-1:45 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.
[2021-12-15 16:00:30.984 algo-1:45 INFO hook.py:192] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.
[2021-12-15 16:00:30.984 algo-1:45 INFO hook.py:237] Saving to /opt/ml/output/tensors
[2021-12-15 16:00:30.984 algo-1:45 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.
Starting Model Training
Epoch: 0
[2021-12-15 16:00:31.250 algo-1:45 INFO hook.py:382] Monitoring the collections: relu_input, losses, CrossEntropyLoss_output_0
[2021-12-15 16:00:31.252 algo-1:45 INFO hook.py:443] Hook is writing from the hook with pid: 45
LossNotDecreasing: InProgress
Overfit: InProgress
Overtraining: Error
PoorWeightInitialization: InProgress
LossNotDecreasing: InProgress
Overfit: InProgress
Overtraining: Error
PoorWeightInitialization: Error
train loss: 64.0000, acc: 7.0000, best loss: 1000000.0000
valid loss: 50.0000, acc: 7.0000, best loss: 50.0000
Testing Model
Testing Loss: 49.0
Testing Accuracy: 7.0
Saving Model
2021-12-15 16:19:32,247 sagemaker-training-toolkit INFO     Reporting training SUCCESS

2021-12-15 16:20:38 Uploading - Uploading generated training model
2021-12-15 16:20:38 Completed - Training job completed
Training seconds: 1359
Billable seconds: 1359

TODO: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?
TODO: If not, suppose there was an error. What would that error look like and how would you have fixed it?

from smdebug.trials import create_trial
from smdebug.core.modes import ModeKeys

trial = create_trial(estimator.latest_job_debugger_artifacts_path())

trial.tensor_names()

[2021-12-15 16:22:11.617 pytorch-1-4-cpu-py36-ml-t3-medium-2c593f76fb32663ae068a7fdddb5:33 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/debug-output
[2021-12-15 16:22:11.956 pytorch-1-4-cpu-py36-ml-t3-medium-2c593f76fb32663ae068a7fdddb5:33 WARNING s3handler.py:183] Encountered the exception ('Connection broken: IncompleteRead(0 bytes read, 4377 more expected)', IncompleteRead(0 bytes read, 4377 more expected)) while reading s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/debug-output/index/000000000/000000000070_worker_0.json . Will retry now
[2021-12-15 16:22:14.122 pytorch-1-4-cpu-py36-ml-t3-medium-2c593f76fb32663ae068a7fdddb5:33 INFO trial.py:198] Training has ended, will refresh one final time in 1 sec.
[2021-12-15 16:22:15.144 pytorch-1-4-cpu-py36-ml-t3-medium-2c593f76fb32663ae068a7fdddb5:33 INFO trial.py:210] Loaded all steps

['CrossEntropyLoss_output_0',
 'layer1.0.relu_input_0',
 'layer1.0.relu_input_1',
 'layer1.0.relu_input_2',
 'layer1.1.relu_input_0',
 'layer1.1.relu_input_1',
 'layer1.1.relu_input_2',
 'layer1.2.relu_input_0',
 'layer1.2.relu_input_1',
 'layer1.2.relu_input_2',
 'layer2.0.relu_input_0',
 'layer2.0.relu_input_1',
 'layer2.0.relu_input_2',
 'layer2.1.relu_input_0',
 'layer2.1.relu_input_1',
 'layer2.1.relu_input_2',
 'layer2.2.relu_input_0',
 'layer2.2.relu_input_1',
 'layer2.2.relu_input_2',
 'layer2.3.relu_input_0',
 'layer2.3.relu_input_1',
 'layer2.3.relu_input_2',
 'layer3.0.relu_input_0',
 'layer3.0.relu_input_1',
 'layer3.0.relu_input_2',
 'layer3.1.relu_input_0',
 'layer3.1.relu_input_1',
 'layer3.1.relu_input_2',
 'layer3.2.relu_input_0',
 'layer3.2.relu_input_1',
 'layer3.2.relu_input_2',
 'layer3.3.relu_input_0',
 'layer3.3.relu_input_1',
 'layer3.3.relu_input_2',
 'layer3.4.relu_input_0',
 'layer3.4.relu_input_1',
 'layer3.4.relu_input_2',
 'layer3.5.relu_input_0',
 'layer3.5.relu_input_1',
 'layer3.5.relu_input_2',
 'layer4.0.relu_input_0',
 'layer4.0.relu_input_1',
 'layer4.0.relu_input_2',
 'layer4.1.relu_input_0',
 'layer4.1.relu_input_1',
 'layer4.1.relu_input_2',
 'layer4.2.relu_input_0',
 'layer4.2.relu_input_1',
 'layer4.2.relu_input_2',
 'relu_input_0']

len(trial.tensor("CrossEntropyLoss_output_0").steps(mode=ModeKeys.TRAIN))

25

steps_eval, vals_eval = get_data(trial, tensor_name="CrossEntropyLoss_output_0", mode=ModeKeys.EVAL)
vals_eval

[array(1.4227208, dtype=float32),
 array(1.560559, dtype=float32),
 array(1.5754172, dtype=float32),
 array(1.4813844, dtype=float32),
 array(1.5050349, dtype=float32),
 array(1.4768227, dtype=float32),
 array(1.4656041, dtype=float32),
 array(1.5749809, dtype=float32),
 array(1.5780169, dtype=float32),
 array(1.5166268, dtype=float32),
 array(1.498223, dtype=float32),
 array(1.5458567, dtype=float32),
 array(1.6439044, dtype=float32),
 array(1.4869624, dtype=float32),
 array(1.7200103, dtype=float32),
 array(1.4923093, dtype=float32),
 array(1.55777, dtype=float32),
 array(1.5773132, dtype=float32),
 array(1.4713074, dtype=float32),
 array(1.6032724, dtype=float32),
 array(1.5812162, dtype=float32),
 array(1.4553457, dtype=float32),
 array(1.5522587, dtype=float32),
 array(1.5452143, dtype=float32),
 array(1.4703974, dtype=float32),
 array(1.6772366, dtype=float32),
 array(1.5749568, dtype=float32),
 array(1.5767205, dtype=float32),
 array(1.5694741, dtype=float32),
 array(1.522675, dtype=float32),
 array(1.6356226, dtype=float32),
 array(1.5619519, dtype=float32),
 array(1.5376881, dtype=float32),
 array(1.4533415, dtype=float32),
 array(1.5070882, dtype=float32),
 array(1.5827813, dtype=float32),
 array(1.6868055, dtype=float32),
 array(1.569221, dtype=float32),
 array(1.5856417, dtype=float32),
 array(1.5734353, dtype=float32),
 array(1.475636, dtype=float32),
 array(1.7220635, dtype=float32),
 array(1.5689343, dtype=float32),
 array(1.7264173, dtype=float32),
 array(1.6508616, dtype=float32),
 array(1.657555, dtype=float32),
 array(1.4819788, dtype=float32),
 array(1.6213244, dtype=float32),
 array(1.5437348, dtype=float32),
 array(1.6162869, dtype=float32),
 array(1.6095437, dtype=float32),
 array(1.477119, dtype=float32),
 array(1.4844656, dtype=float32),
 array(1.6326078, dtype=float32),
 array(1.484312, dtype=float32),
 array(1.505629, dtype=float32),
 array(1.538775, dtype=float32),
 array(1.4286283, dtype=float32),
 array(1.4775822, dtype=float32),
 array(1.6008503, dtype=float32),
 array(1.6337922, dtype=float32),
 array(1.6463678, dtype=float32),
 array(1.4443002, dtype=float32),
 array(1.5737177, dtype=float32),
 array(1.5132072, dtype=float32),
 array(1.4877112, dtype=float32),
 array(1.5222899, dtype=float32),
 array(1.5082818, dtype=float32),
 array(1.5598568, dtype=float32),
 array(1.652561, dtype=float32),
 array(1.5715588, dtype=float32),
 array(1.5313306, dtype=float32),
 array(1.5358421, dtype=float32),
 array(1.5462513, dtype=float32),
 array(1.5224199, dtype=float32),
 array(1.5876588, dtype=float32),
 array(1.6763564, dtype=float32),
 array(1.5588579, dtype=float32),
 array(1.5839381, dtype=float32),
 array(1.5720243, dtype=float32),
 array(1.4462565, dtype=float32),
 array(1.5580144, dtype=float32)]

def get_data(trial, tensor_name, mode):
    tensor = trial.tensor(tensor_name)
    steps = tensor.steps(mode=mode)
    vals = []
    for s in steps:
        vals.append(tensor.value(s, mode=mode))
    return steps, vals

steps_train, vals_train = get_data(trial, tensor_name="CrossEntropyLoss_output_0", mode=ModeKeys.TRAIN)
vals_train

[array(1.615513, dtype=float32),
 array(2.6290715, dtype=float32),
 array(1.6821663, dtype=float32),
 array(1.5230663, dtype=float32),
 array(1.602185, dtype=float32),
 array(1.6060736, dtype=float32),
 array(1.6029261, dtype=float32),
 array(1.5270646, dtype=float32),
 array(1.5658673, dtype=float32),
 array(1.549157, dtype=float32),
 array(1.534904, dtype=float32),
 array(1.5872492, dtype=float32),
 array(1.6219609, dtype=float32),
 array(1.7424498, dtype=float32),
 array(1.560359, dtype=float32),
 array(1.5118035, dtype=float32),
 array(1.5371286, dtype=float32),
 array(1.6556954, dtype=float32),
 array(1.5607666, dtype=float32),
 array(1.5324514, dtype=float32),
 array(1.63818, dtype=float32),
 array(1.5950547, dtype=float32),
 array(1.6270021, dtype=float32),
 array(1.5910853, dtype=float32),
 array(1.6698141, dtype=float32)]

import matplotlib.pyplot as plt


from smdebug.trials import create_trial
from smdebug import modes
import numpy as np
import matplotlib.pyplot as plt


# Get the tensors from S3
s3_output_path = estimator.latest_job_debugger_artifacts_path()

# Create a Trial https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md#Trial
trial = create_trial(s3_output_path)

# Get all the tensor names
trial.tensor_names()

# Get the values of the tensor `val_acc`for mode GLOBAL (validation accuracy)
values = trial.tensor("CrossEntropyLoss_output_0").values(modes.GLOBAL)

# Convert it to numpy array
values_eval = np.array(list(values.items()))

fig = plt.figure()
plt.plot(values_eval[:, 1])
fig.suptitle('Validation Loss', fontsize=20)
plt.xlabel('Intervals of sampling', fontsize=18)
plt.ylabel('Loss', fontsize=16)
fig.savefig('temp.jpg')

[2021-12-15 16:22:39.644 pytorch-1-4-cpu-py36-ml-t3-medium-2c593f76fb32663ae068a7fdddb5:33 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/debug-output
[2021-12-15 16:22:39.871 pytorch-1-4-cpu-py36-ml-t3-medium-2c593f76fb32663ae068a7fdddb5:33 WARNING s3handler.py:183] Encountered the exception ('Connection broken: IncompleteRead(0 bytes read, 4377 more expected)', IncompleteRead(0 bytes read, 4377 more expected)) while reading s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/debug-output/index/000000000/000000000070_worker_0.json . Will retry now
[2021-12-15 16:22:41.795 pytorch-1-4-cpu-py36-ml-t3-medium-2c593f76fb32663ae068a7fdddb5:33 INFO trial.py:198] Training has ended, will refresh one final time in 1 sec.
[2021-12-15 16:22:42.819 pytorch-1-4-cpu-py36-ml-t3-medium-2c593f76fb32663ae068a7fdddb5:33 INFO trial.py:210] Loaded all steps

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import host_subplot

def plot_tensor(trial, tensor_name):
    steps_train, vals_train = get_data(trial, tensor_name, mode=ModeKeys.TRAIN)
    print("loaded TRAIN data")
    steps_eval, vals_eval = get_data(trial, tensor_name, mode=ModeKeys.EVAL)
    print("loaded EVAL data")

    fig = plt.figure(figsize=(10, 7))
    host = host_subplot(111)

    par = host.twiny()

    host.set_xlabel("Steps (TRAIN)")
    par.set_xlabel("Steps (EVAL)")
    host.set_ylabel(tensor_name)

    (p1,) = host.plot(steps_train, vals_train, label=tensor_name)
    print("completed TRAIN plot")
    (p2,) = par.plot(steps_eval, vals_eval, label="val_" + tensor_name)
    print("completed EVAL plot")
    leg = plt.legend()

    host.xaxis.get_label().set_color(p1.get_color())
    leg.texts[0].set_color(p1.get_color())

    par.xaxis.get_label().set_color(p2.get_color())
    leg.texts[1].set_color(p2.get_color())

    plt.ylabel(tensor_name)

    plt.show()

plot_tensor(trial, "CrossEntropyLoss_output_0")

loaded TRAIN data
loaded EVAL data
completed TRAIN plot
completed EVAL plot

# TODO: Display the profiler output
import os

rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + "/rule-output"
print(f"You will find the profiler report in {rule_output_path}")
!aws s3 ls {rule_output_path} --recursive

## copy to local
!aws s3 cp {rule_output_path} ./ --recursive

You will find the profiler report in s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output
2021-12-15 16:20:32     329710 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-report.html
2021-12-15 16:20:32     171072 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-report.ipynb
2021-12-15 16:20:28        192 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/BatchSize.json
2021-12-15 16:20:28        200 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json
2021-12-15 16:20:28        126 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/Dataloader.json
2021-12-15 16:20:28        127 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json
2021-12-15 16:20:28        199 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json
2021-12-15 16:20:28        119 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json
2021-12-15 16:20:28        151 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json
2021-12-15 16:20:28        179 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json
2021-12-15 16:20:28        133 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json
2021-12-15 16:20:28        477 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json
2021-12-15 16:20:28        156 Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/StepOutlier.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/Dataloader.json to ProfilerReport/profiler-output/profiler-reports/Dataloader.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json to ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json to ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/BatchSize.json to ProfilerReport/profiler-output/profiler-reports/BatchSize.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json to ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json to ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json to ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json to ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-report.html to ProfilerReport/profiler-output/profiler-report.html
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json to ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-report.ipynb to ProfilerReport/profiler-output/profiler-report.ipynb
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/StepOutlier.json to ProfilerReport/profiler-output/profiler-reports/StepOutlier.json
download: s3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json to ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json

profiler_report_name = [
    rule["RuleConfigurationName"]
    for rule in estimator.latest_training_job.rule_job_summary()
    if "Profiler" in rule["RuleConfigurationName"]
][0]

import IPython

IPython.display.HTML(filename=profiler_report_name + "/profiler-output/profiler-report.html")

profiler-report
SageMaker Debugger Profiling Report

SageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive.

Legal disclaimer: This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.
In [4]:

# Parameters
processing_job_arn = "arn:aws:sagemaker:us-east-1:701620947472:processing-job/training-job-2021-12-15-15-profilerreport-58070356"

Training job summary
System usage statistics
Framework metrics summary
Rules summary

The following table shows a profiling summary of the Debugger built-in rules. The table is sorted by the rules that triggered the most frequently. During your training job, the CPUBottleneck rule was the most frequently triggered. It processed 2644 datapoints and was triggered 0 times.
	Description 	Recommendation 	Number of times rule triggered 	Number of datapoints 	Rule parameters
CPUBottleneck 	Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent. 	Consider increasing the number of data loaders or applying data pre-fetching. 	0 	2644 	threshold:50
cpu_threshold:90
gpu_threshold:10
patience:1000
Dataloader 	Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU. 	Change the number of data loader processes. 	0 	0 	min_threshold:70
max_threshold:200
GPUMemoryIncrease 	Measures the average GPU memory footprint and triggers if there is a large increase. 	Choose a larger instance type with more memory if footprint is close to maximum available memory. 	0 	0 	increase:5
patience:1000
window:10
IOBottleneck 	Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent. 	Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance. 	0 	2644 	threshold:50
io_threshold:50
gpu_threshold:10
patience:1000
BatchSize 	Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization. 	The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size. 	0 	2641 	cpu_threshold_p95:70
gpu_threshold_p95:70
gpu_memory_threshold_p95:70
patience:1000
window:500
LowGPUUtilization 	Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size. 	Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size. 	0 	0 	threshold_p95:70
threshold_p5:10
window:500
patience:1000
StepOutlier 	Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues. 	Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers. 	0 	0 	threshold:3
mode:None
n_outliers:10
stddev:3
MaxInitializationTime 	Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes. 	Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework. 	0 	0 	threshold:20
LoadBalancing 	Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization. 	Choose a different distributed training strategy or a different distributed training framework. 	0 	0 	threshold:0.2
patience:1000
Analyzing the training loop
Step duration analysis
GPU utilization analysis

Usage per GPU

Workload balancing
Dataloading analysis
Batch size
CPU bottlenecks
I/O bottlenecks
GPU memory
Model Deploying and Querying

TODO: Can you deploy your model to an endpoint and then query that endpoint to get a result?

estimator.model_data

's3://sagemaker-us-east-1-701620947472/Training-job-2021-12-15-15-55-11-903/output/model.tar.gz'

model_location=estimator.model_data

from sagemaker.pytorch import PyTorchModel
from sagemaker.predictor import Predictor

jpeg_serializer = sagemaker.serializers.IdentitySerializer("image/jpeg")
json_deserializer = sagemaker.deserializers.JSONDeserializer()

class ImagePredictor(Predictor):
    def __init__(self, endpoint_name, sagemaker_session):
        super(ImagePredictor, self).__init__(
            endpoint_name,
            sagemaker_session=sagemaker_session,
            serializer=jpeg_serializer,
            deserializer=json_deserializer,
        )

pytorch_model = PyTorchModel(model_data=model_location, role=role, entry_point='inference.py',py_version='py3',
                             framework_version='1.4',
                             predictor_cls=ImagePredictor)

predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.m5.large')

-----!

request_dict={ "url": "https://aft-vbi-pds.s3.amazonaws.com/bin-images/777.jpg" }
img_bytes = requests.get(request_dict['url']).content
Image.open(io.BytesIO(img_bytes))

from IPython.display import Image

response=predictor.predict(json.dumps(request_dict), initial_args={"ContentType": "application/json"})

response[0]

[-0.14856533706188202,
 0.1561780571937561,
 0.10614462196826935,
 0.02995668724179268,
 -0.03836285322904587]

np.argmax(response, 1)

array([1])

Cheaper Training and Cost Analysis

TODO: Can you perform a cost analysis of your system and then use spot instances to lessen your model training cost?
Cost Analysis

Sagmaker vs EC2 training
Sagemaker
Sagemaker cost items are as follows

    GB-Mo of Training Job ML storage
    Studio-Notebook ml.t3.medium hour in US East (N. Virginia)
    Training ml.m5.xlarge hour in US East (N. Virginia)
    SageMaker Studio Notebook Instance ml.g4dn.xlarge per hour

Overall cost for SageMaker usage - $6.20

Image("sage.PNG")

EC2 Model cost Items

    Amazon Elastic Compute Cloud running Linux/UNIX - Demand Linux t2.medium Instance

    General Purpose SSD (gp2) provisioned storage

Overall cost for SageMaker usage - $0.11

Image("ec.PNG")

Model training cost in EC2 is less compared to SageMaker. However, higher instance cost and usage will be a factor on considering the workspace
Multi-Instance Training
Number of instance - 2

multi_instance = PyTorch(
    base_job_name="Multi-instance-job",
    sagemaker_session=sagemaker_session,
    entry_point="train.py",
    role=role,
    framework_version="1.4.0",
    py_version="py3",
    instance_count=2,
    instance_type="ml.m5.xlarge",
    hyperparameters=hyperparameters,
    rules = rules,
    debugger_hook_config=hook_config,
    profiler_config=profiler_config,
)
multi_instance.fit({"training": "s3://udacitycapstone9/"}, wait=True)

2021-12-15 16:55:44 Starting - Starting the training job...
2021-12-15 16:55:53 Starting - Launching requested ML instancesLossNotDecreasing: InProgress
Overfit: InProgress
Overtraining: InProgress
PoorWeightInitialization: InProgress
LowGPUUtilization: InProgress
ProfilerReport: InProgress
......
2021-12-15 16:57:03 Starting - Preparing the instances for training.........
2021-12-15 16:58:42 Downloading - Downloading input data............
2021-12-15 17:00:44 Training - Downloading the training image.bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
2021-12-15 17:00:50,000 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training
2021-12-15 17:00:50,004 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 17:00:50,016 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.
bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
2021-12-15 17:00:51,497 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training
2021-12-15 17:00:51,501 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 17:00:51,512 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.
2021-12-15 17:00:54,553 sagemaker_pytorch_container.training INFO     Invoking user training script.
2021-12-15 17:00:56,260 sagemaker_pytorch_container.training INFO     Invoking user training script.
2021-12-15 17:00:56,626 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. 
Generating setup.py
2021-12-15 17:00:56,626 sagemaker-containers INFO     Generating setup.cfg
2021-12-15 17:00:56,626 sagemaker-containers INFO     Generating MANIFEST.in
2021-12-15 17:00:56,626 sagemaker-containers INFO     Installing module with the following command:
/opt/conda/bin/python3.6 -m pip install . 
Processing /tmp/tmpa7pvqi73/module_dir
Building wheels for collected packages: default-user-module-name
  Building wheel for default-user-module-name (setup.py): started
  Building wheel for default-user-module-name (setup.py): finished with status 'done'
  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=6569 sha256=7acd19074b0c1ff415daee4b0839fe2a2547d8762c8f83f6f26922a8d02495dc
  Stored in directory: /tmp/pip-ephem-wheel-cache-qy97om9d/wheels/3e/67/87/3a5b50631078065a7ba95242f521a218d255998c21956185fb
Successfully built default-user-module-name
Installing collected packages: default-user-module-name
Successfully installed default-user-module-name-1.0.0
2021-12-15 17:00:57,995 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. 
Generating setup.py
2021-12-15 17:00:57,996 sagemaker-containers INFO     Generating setup.cfg
2021-12-15 17:00:57,996 sagemaker-containers INFO     Generating MANIFEST.in
2021-12-15 17:00:57,996 sagemaker-containers INFO     Installing module with the following command:
/opt/conda/bin/python3.6 -m pip install . 
Processing /tmp/tmp4sc_yf76/module_dir
2021-12-15 17:00:58,993 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 17:00:59,005 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 17:00:59,022 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 17:00:59,032 sagemaker-containers INFO     Invoking user script
Training Env:
{
    "additional_framework_parameters": {},
    "channel_input_dirs": {
        "training": "/opt/ml/input/data/training"
    },
    "current_host": "algo-1",
    "framework_module": "sagemaker_pytorch_container.training:main",
    "hosts": [
        "algo-1",
        "algo-2"
    ],
    "hyperparameters": {
        "batch_size": 128,
        "epochs": 15,
        "learning_rate": 0.01745493566174512
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {
        "training": {
            "TrainingInputMode": "File",
            "S3DistributionType": "FullyReplicated",
            "RecordWrapperType": "None"
        }
    },
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "Multi-instance-job-2021-12-15-16-55-43-939",
    "log_level": 20,
    "master_hostname": "algo-1",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://sagemaker-us-east-1-701620947472/Multi-instance-job-2021-12-15-16-55-43-939/source/sourcedir.tar.gz",
    "module_name": "train",
    "network_interface_name": "eth0",
    "num_cpus": 4,
    "num_gpus": 0,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1",
        "hosts": [
            "algo-1",
            "algo-2"
        ],
        "network_interface_name": "eth0"
    },
    "user_entry_point": "train.py"
}
Environment variables:
SM_HOSTS=["algo-1","algo-2"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"batch_size":128,"epochs":15,"learning_rate":0.01745493566174512}
SM_USER_ENTRY_POINT=train.py
SM_FRAMEWORK_PARAMS={}
SM_RESOURCE_CONFIG={"current_host":"algo-1","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"}
SM_INPUT_DATA_CONFIG={"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=["training"]
SM_CURRENT_HOST=algo-1
SM_MODULE_NAME=train
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=4
SM_NUM_GPUS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://sagemaker-us-east-1-701620947472/Multi-instance-job-2021-12-15-16-55-43-939/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{},"channel_input_dirs":{"training":"/opt/ml/input/data/training"},"current_host":"algo-1","framework_module":"sagemaker_pytorch_container.training:main","hosts":["algo-1","algo-2"],"hyperparameters":{"batch_size":128,"epochs":15,"learning_rate":0.01745493566174512},"input_config_dir":"/opt/ml/input/config","input_data_config":{"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}},"input_dir":"/opt/ml/input","is_master":true,"job_name":"Multi-instance-job-2021-12-15-16-55-43-939","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-east-1-701620947472/Multi-instance-job-2021-12-15-16-55-43-939/source/sourcedir.tar.gz","module_name":"train","network_interface_name":"eth0","num_cpus":4,"num_gpus":0,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"},"user_entry_point":"train.py"}
SM_USER_ARGS=["--batch_size","128","--epochs","15","--learning_rate","0.01745493566174512"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_CHANNEL_TRAINING=/opt/ml/input/data/training
SM_HP_BATCH_SIZE=128
SM_HP_EPOCHS=15
SM_HP_LEARNING_RATE=0.01745493566174512
PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages
Invoking script with the following command:
/opt/conda/bin/python3.6 train.py --batch_size 128 --epochs 15 --learning_rate 0.01745493566174512
Building wheels for collected packages: default-user-module-name
  Building wheel for default-user-module-name (setup.py): started
  Building wheel for default-user-module-name (setup.py): finished with status 'done'
  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=6569 sha256=a6547119b7f275b02bce725e840d92e3ce93e4f413d9488240a8ea75f93523ad
  Stored in directory: /tmp/pip-ephem-wheel-cache-uq_prl8t/wheels/e4/50/34/d19ce039fa09b7bc1210b7db15316c01762b7f9d603307dff1
Successfully built default-user-module-name
Namespace(batch_size=128, data='/opt/ml/input/data/training', epochs=15, learning_rate=0.01745493566174512, model_dir='/opt/ml/model', output_dir='/opt/ml/output/data')
Hyperparameters are LR: 0.01745493566174512, Batch Size: 128
Data Paths: /opt/ml/input/data/training
Installing collected packages: default-user-module-name
Successfully installed default-user-module-name-1.0.0
2021-12-15 17:01:00,347 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 17:01:00,360 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 17:01:00,373 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)
2021-12-15 17:01:00,384 sagemaker-containers INFO     Invoking user script
Training Env:
{
    "additional_framework_parameters": {},
    "channel_input_dirs": {
        "training": "/opt/ml/input/data/training"
    },
    "current_host": "algo-2",
    "framework_module": "sagemaker_pytorch_container.training:main",
    "hosts": [
        "algo-1",
        "algo-2"
    ],
    "hyperparameters": {
        "batch_size": 128,
        "epochs": 15,
        "learning_rate": 0.01745493566174512
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {
        "training": {
            "TrainingInputMode": "File",
            "S3DistributionType": "FullyReplicated",
            "RecordWrapperType": "None"
        }
    },
    "input_dir": "/opt/ml/input",
    "is_master": false,
    "job_name": "Multi-instance-job-2021-12-15-16-55-43-939",
    "log_level": 20,
    "master_hostname": "algo-1",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://sagemaker-us-east-1-701620947472/Multi-instance-job-2021-12-15-16-55-43-939/source/sourcedir.tar.gz",
    "module_name": "train",
    "network_interface_name": "eth0",
    "num_cpus": 4,
    "num_gpus": 0,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-2",
        "hosts": [
            "algo-1",
            "algo-2"
        ],
        "network_interface_name": "eth0"
    },
    "user_entry_point": "train.py"
}
Environment variables:
SM_HOSTS=["algo-1","algo-2"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"batch_size":128,"epochs":15,"learning_rate":0.01745493566174512}
SM_USER_ENTRY_POINT=train.py
SM_FRAMEWORK_PARAMS={}
SM_RESOURCE_CONFIG={"current_host":"algo-2","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"}
SM_INPUT_DATA_CONFIG={"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=["training"]
SM_CURRENT_HOST=algo-2
SM_MODULE_NAME=train
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=4
SM_NUM_GPUS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://sagemaker-us-east-1-701620947472/Multi-instance-job-2021-12-15-16-55-43-939/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{},"channel_input_dirs":{"training":"/opt/ml/input/data/training"},"current_host":"algo-2","framework_module":"sagemaker_pytorch_container.training:main","hosts":["algo-1","algo-2"],"hyperparameters":{"batch_size":128,"epochs":15,"learning_rate":0.01745493566174512},"input_config_dir":"/opt/ml/input/config","input_data_config":{"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}},"input_dir":"/opt/ml/input","is_master":false,"job_name":"Multi-instance-job-2021-12-15-16-55-43-939","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-east-1-701620947472/Multi-instance-job-2021-12-15-16-55-43-939/source/sourcedir.tar.gz","module_name":"train","network_interface_name":"eth0","num_cpus":4,"num_gpus":0,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-2","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"},"user_entry_point":"train.py"}
SM_USER_ARGS=["--batch_size","128","--epochs","15","--learning_rate","0.01745493566174512"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_CHANNEL_TRAINING=/opt/ml/input/data/training
SM_HP_BATCH_SIZE=128
SM_HP_EPOCHS=15
SM_HP_LEARNING_RATE=0.01745493566174512
PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages
Invoking script with the following command:
/opt/conda/bin/python3.6 train.py --batch_size 128 --epochs 15 --learning_rate 0.01745493566174512
[2021-12-15 17:01:01.495 algo-1:46 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.
[2021-12-15 17:01:01.496 algo-1:46 INFO hook.py:192] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.
[2021-12-15 17:01:01.496 algo-1:46 INFO hook.py:237] Saving to /opt/ml/output/tensors
[2021-12-15 17:01:01.496 algo-1:46 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.
Starting Model Training
Epoch: 0
[2021-12-15 17:01:01.793 algo-1:46 INFO hook.py:382] Monitoring the collections: losses, relu_input, CrossEntropyLoss_output_0
[2021-12-15 17:01:01.794 algo-1:46 INFO hook.py:443] Hook is writing from the hook with pid: 46
Namespace(batch_size=128, data='/opt/ml/input/data/training', epochs=15, learning_rate=0.01745493566174512, model_dir='/opt/ml/model', output_dir='/opt/ml/output/data')
Hyperparameters are LR: 0.01745493566174512, Batch Size: 128
Data Paths: /opt/ml/input/data/training
[2021-12-15 17:01:03.164 algo-2:45 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.
[2021-12-15 17:01:03.165 algo-2:45 INFO hook.py:192] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.
[2021-12-15 17:01:03.165 algo-2:45 INFO hook.py:237] Saving to /opt/ml/output/tensors
[2021-12-15 17:01:03.165 algo-2:45 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.
Starting Model Training
Epoch: 0
[2021-12-15 17:01:03.551 algo-2:45 INFO hook.py:382] Monitoring the collections: losses, relu_input, CrossEntropyLoss_output_0
[2021-12-15 17:01:03.553 algo-2:45 INFO hook.py:443] Hook is writing from the hook with pid: 45

2021-12-15 17:01:18 Training - Training image download completed. Training in progress.LossNotDecreasing: InProgress
Overfit: Error
Overtraining: InProgress
PoorWeightInitialization: InProgress
LossNotDecreasing: InProgress
Overfit: Error
Overtraining: InProgress
PoorWeightInitialization: Error
train loss: 60.0000, acc: 8.0000, best loss: 1000000.0000
train loss: 59.0000, acc: 8.0000, best loss: 1000000.0000
valid loss: 50.0000, acc: 7.0000, best loss: 50.0000
Testing Model
valid loss: 50.0000, acc: 7.0000, best loss: 50.0000
Testing Model
Testing Loss: 50.0
Testing Accuracy: 8.0
Saving Model
2021-12-15 17:20:10,983 sagemaker-training-toolkit INFO     Reporting training SUCCESS
Testing Loss: 50.0
Testing Accuracy: 8.0
Saving Model
2021-12-15 17:20:12,932 sagemaker-training-toolkit INFO     Reporting training SUCCESS

2021-12-15 17:21:16 Uploading - Uploading generated training model
2021-12-15 17:21:48 Completed - Training job completed
LossNotDecreasing: NoIssuesFound
Overfit: Error
Overtraining: NoIssuesFound
PoorWeightInitialization: Error
LowGPUUtilization: NoIssuesFound
ProfilerReport: NoIssuesFound
Training seconds: 2748
Billable seconds: 2748

 

